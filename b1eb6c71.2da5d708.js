(window.webpackJsonp=window.webpackJsonp||[]).push([[359],{416:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return s})),n.d(t,"rightToc",(function(){return l})),n.d(t,"default",(function(){return p}));var a=n(2),o=n(7),r=(n(0),n(561)),i={id:"ceph",title:"Ceph"},s={unversionedId:"service/storage/ceph",id:"service/storage/ceph",isDocsHomePage:!1,title:"Ceph",description:"Ceph",source:"@site/contents/notes/service/storage/ceph.md",slug:"/service/storage/ceph",permalink:"/notes/service/storage/ceph",version:"current",sidebar:"docs",previous:{title:"MinIO Kubernetes Operator",permalink:"/notes/service/storage/minio-operator"},next:{title:"Ceph \u672f\u8bed",permalink:"/notes/service/storage/ceph-glossary"}},l=[{value:"Tips",id:"tips",children:[]},{value:"Troubleshooting",id:"troubleshooting",children:[]},{value:"\u6280\u672f\u6bd4\u8f83",id:"\u6280\u672f\u6bd4\u8f83",children:[{value:"vs HDFS",id:"vs-hdfs",children:[]}]},{value:"\u547d\u4ee4\u884c",id:"\u547d\u4ee4\u884c",children:[]},{value:"FAQ",id:"faq",children:[{value:"HEALTH_ERR 64 pgs are stuck inactive for more than 300 seconds; 64 pgs stuck inactive",id:"health_err-64-pgs-are-stuck-inactive-for-more-than-300-seconds-64-pgs-stuck-inactive",children:[]},{value:"ERROR: osd init failed: (36) File name too long",id:"error-osd-init-failed-36-file-name-too-long",children:[]}]}],c={rightToc:l};function p(e){var t=e.components,n=Object(o.a)(e,["components"]);return Object(r.b)("wrapper",Object(a.a)({},c,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("h1",{id:"ceph"},"Ceph"),Object(r.b)("h2",{id:"tips"},"Tips"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"\u9002\u7528\u573a\u666f",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"\u5355\u72ec\u5b58\u50a8\u96c6\u7fa4"),Object(r.b)("li",{parentName:"ul"},"\u5927\u5b58\u50a8\u96c6\u7fa4 - TB+ PB+"))),Object(r.b)("li",{parentName:"ul"},"\u4e0d\u9002\u7528\u573a\u666f",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"\u4e0d\u592a\u9002\u5408\u4e8e\u76f4\u63a5\u90e8\u7f72\u5230 k8s \u5c0f\u96c6\u7fa4\u4f5c\u4e3a\u5fae\u670d\u52a1\u5b58\u50a8\u4f7f\u7528"),Object(r.b)("li",{parentName:"ul"},"\u7b80\u5355\u7684\u5206\u5e03\u5f0f\u6587\u4ef6\u7cfb\u7edf"))),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/ceph/ceph"}),"ceph/ceph"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"http://docs.ceph.com/docs/master/"}),"\u6587\u6863")))),Object(r.b)("li",{parentName:"ul"},"imixs ",Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"https://github.com/imixs/imixs-cloud/blob/master/doc/CEPH.md"}),"Ceph")),Object(r.b)("li",{parentName:"ul"},"\u9ed8\u8ba4\u65e5\u5fd7\u8def\u5f84\u4e3a ",Object(r.b)("inlineCode",{parentName:"li"},"/var/lib/ceph/osd/$cluster-$id/journal"),", \u53ef\u5c06\u8be5\u6587\u4ef6\u6302\u5728\u5230\u5176\u5b83\u78c1\u76d8\u4ee5\u589e\u52a0\u6027\u80fd"),Object(r.b)("li",{parentName:"ul"},"Ceph \u4f7f\u7528\u526f\u672c\u6216 EC \u6765\u4fdd\u62a4\u6570\u636e"),Object(r.b)("li",{parentName:"ul"},"CephFS \u9ed8\u8ba4\u7aef\u53e3\u4e3a 6789"),Object(r.b)("li",{parentName:"ul"},"\u53c2\u8003",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"https://www.youtube.com/watch?v=VFeDB7PQHZ4"}),"Bluestore vs. Filestore: A Functional Comparison and a Benchmark"))))),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"\u6700\u4f73\u5b9e\u8df5")),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"\u6bcf\u4e2a OSD \u9884\u7559 2G \u5185\u5b58,\u6bcf\u4e2a OSD \u4e00\u4e2a\u78c1\u76d8,\u5197\u4f59 NIC"),Object(r.b)("li",{parentName:"ul"},"\u5982\u679c\u6709 SSD, \u53ef\u5c06 SSD \u4f5c\u4e3a\u65e5\u5fd7\u78c1\u76d8, \u4f7f\u7528\u673a\u68b0\u76d8\u4f5c\u4e3a\u5b58\u50a8\u78c1\u76d8, \u5982\u679c SSD \u8f83\u5c11,\u53ef\u591a\u4e2a OSD \u5171\u7528\u4e00\u4e2a SSD(\u4e00\u4e2a SSD \u635f\u574f\u5f71\u54cd\u591a\u4e2a OSD)"),Object(r.b)("li",{parentName:"ul"},"\u4f7f\u7528 SSD \u524d\u6700\u597d\u4e8b\u5148\u6d4b\u8bd5\u51fa\u987a\u5e8f\u548c\u968f\u673a\u8bfb\u5199\u7684\u6027\u80fd"),Object(r.b)("li",{parentName:"ul"},"MON \u4f4d\u4e8e\u5355\u72ec\u670d\u52a1\u5668, \u4e0d\u9700\u8981\u592a\u591a\u7684 RAM \u548c\u78c1\u76d8,\u5197\u4f59 NIC"),Object(r.b)("li",{parentName:"ul"},"\u5982\u679c\u5355\u4e2a\u4e3b\u673a\u4e0a OSD \u8f83\u591a(\u4f8b\u5982 >20),\u5efa\u8bae\u589e\u5927\u7cfb\u7edf\u7ebf\u7a0b\u6570 ",Object(r.b)("inlineCode",{parentName:"li"},"kernel.pid_max = 4194303")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"http://docs.ceph.com/docs/master/start/hardware-recommendations/"}),"\u63a8\u8350\u786c\u4ef6"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"\u4e0d\u63a8\u8350\u548c\u8ba1\u7b97\u96c6\u7fa4\u4e00\u8d77\u90e8\u7f72 - k8s"),Object(r.b)("li",{parentName:"ul"},"ceph-mds - metadata",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"4 CPU+"),Object(r.b)("li",{parentName:"ul"},"1GB+"))),Object(r.b)("li",{parentName:"ul"},"ceph-osd",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"2 CPU+"),Object(r.b)("li",{parentName:"ul"},"\u9ed8\u8ba4 4GB\uff0c 2-4GB \u53ef\u7528\uff0c\u4e0d\u63a8\u8350\u5c0f\u4e8e 2GB "),Object(r.b)("li",{parentName:"ul"},"\u5c0f\u6587\u4ef6\u591a\u5efa\u8bae 4GB+\uff0c\u5927\u6570\u636e\u4f20\u8f93\u5efa\u8bae 256GB+"),Object(r.b)("li",{parentName:"ul"},"\u78c1\u76d8\u63a8\u8350 1TB+"),Object(r.b)("li",{parentName:"ul"},"\u4e0d\u63a8\u8350 1\u786c\u76d8 1OSD 1MSD\uff0c\u4e0d\u63a8\u8350 1\u786c\u76d8\u591aOSD"),Object(r.b)("li",{parentName:"ul"},"\u5206\u79bb \u65e5\u5fd7\u3001\u6570\u636e\u3001\u7cfb\u7edf \u76d8"),Object(r.b)("li",{parentName:"ul"},"\u63a8\u8350\u4f7f\u7528 SSD \u4f5c\u4e3a \u65e5\u5fd7 \u76d8"),Object(r.b)("li",{parentName:"ul"},"\u786e\u4fdd\u7f51\u7edc\u5e26\u5bbd\u5927\u4e8e\u603b\u78c1\u76d8\u5e26\u5bbd"),Object(r.b)("li",{parentName:"ul"},"10Gbps+ - 1T \u526f\u672c 20\u5206\u949f"))),Object(r.b)("li",{parentName:"ul"},"ceph-mgr",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"1-2 GB \u5c0f\u96c6\u7fa4\uff0c5-10 GB \u5927\u96c6\u7fa4 - \u968f\u96c6\u7fa4\u6269\u5bb9 - \u57fa\u672c 1G/1T"))),Object(r.b)("li",{parentName:"ul"},"ceph-mon - monitor",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"CPU \u8981\u6c42\u4e0d\u9ad8"),Object(r.b)("li",{parentName:"ul"},"1-2 GB \u5c0f\u96c6\u7fa4\uff0c5-10 GB \u5927\u96c6\u7fa4 - \u540c ceph-mgr"))))),Object(r.b)("li",{parentName:"ul"},"\u53c2\u8003 ",Object(r.b)("a",Object(a.a)({parentName:"li"},{href:"http://docs.ceph.com/docs/master/start/os-recommendations/"}),"\u63a8\u8350\u64cd\u4f5c\u7cfb\u7edf"))),Object(r.b)("table",null,Object(r.b)("thead",{parentName:"table"},Object(r.b)("tr",{parentName:"thead"},Object(r.b)("th",Object(a.a)({parentName:"tr"},{align:null}),Object(r.b)("strong",{parentName:"th"},"\u5bf9\u8c61\u5b58\u50a8/rados")),Object(r.b)("th",Object(a.a)({parentName:"tr"},{align:null}),Object(r.b)("strong",{parentName:"th"},"\u5757\u8bbe\u5907/RBD")),Object(r.b)("th",Object(a.a)({parentName:"tr"},{align:null}),Object(r.b)("strong",{parentName:"th"},"\u6587\u4ef6\u7cfb\u7edf/CephFS")))),Object(r.b)("tbody",{parentName:"table"},Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"REST \u63a5\u53e3"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"Thin-provisioned"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u7b26\u5408 POSIX \u8bed\u4e49")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"S3\u3001Swift \u517c\u5bb9\u63a5\u53e3"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"16 EB"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5143\u6570\u636e\u6570\u636e\u5206\u79bb")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"S3 \u5b50\u57df\u540d"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"Configurable striping"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"Configurable striping")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"S3\u3001Swift \u547d\u540d\u7a7a\u95f4"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5185\u5b58\u7f13\u5b58"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u52a8\u6001\u5e73\u8861")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u7528\u6237\u7ba1\u7406"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5feb\u7167"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5b50\u76ee\u5f55\u5feb\u7167")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u4f7f\u7528\u8ddf\u8e2a"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"COW \u514b\u9686"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"FUSE \u652f\u6301")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"Striped objects"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5185\u6838\u9a71\u52a8\u652f\u6301"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u5185\u6838\u9a71\u52a8\u652f\u6301")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u4e91\u96c6\u6210"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"KVM/libvirt \u652f\u6301"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u53ef\u90e8\u7f72 NFS/CIFS")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u591a\u7ad9\u70b9\u90e8\u7f72"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u540e\u7aef\u4e91\u65b9\u6848\u96c6\u6210"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u53ef\u914d\u5408 Hadoop - \u66ff\u4ee3 HDFS")),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u591a\u7ad9\u70b9\u526f\u672c"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u589e\u91cf\u5907\u4efd"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}))),Object(r.b)("tr",{parentName:"tbody"},Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null})),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}),"\u707e\u96be\u6062\u590d - \u591a\u7ad9\u70b9\u5f02\u6b65\u526f\u672c"),Object(r.b)("td",Object(a.a)({parentName:"tr"},{align:null}))))),Object(r.b)("p",null,Object(r.b)("strong",{parentName:"p"},"\u6240\u6709\u7684\u547d\u4ee4")),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{}),"ceph                   ceph-create-keys       ceph-detect-init       ceph-fuse              ceph-post-file\nceph-authtool          ceph-crush-location    ceph-disk              ceph-mds               ceph-rbdnamer\nceph-bluefs-tool       ceph-debugpack         cephfs-data-scan       ceph-mon               ceph-rest-api\nceph-clsinfo           ceph-dencoder          cephfs-journal-tool    ceph-objectstore-tool  ceph-run\nceph-conf              ceph-deploy            cephfs-table-tool      ceph-osd               ceph-syn\n")),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{className:"language-bash"}),"# \u67e5\u770b\u78c1\u76d8\u7c7b\u578b\nfile -sL /dev/vda1\ndf -T\n\n# XFS \u6d4b\u8bd5\napt-get install xfsprogs\n# \u4f7f\u7528\u6587\u4ef6\u521b\u5efa xfs\ntruncate -s 2G data.vol\n# \u4e5f\u53ef\u6302\u8f7d\u4e3a\u8bbe\u5907\n# losetup /dev/loop5 data.vol\n\n# \u683c\u5f0f\u5316 n \u521b\u5efa\u5206\u533a p \u4e3b\u5206\u533a w \u5199\u5165\nfdisk data.vol\n# \u683c\u5f0f\u5316\u4e3a xfs\nmkfs.xfs -f data.vol\n# \u6302\u8f7d\nmkdir /storage\nmount -t xfs data.vol /storage\n# \u67e5\u770b\u6587\u4ef6\u7cfb\u7edf\ndf -Th /storage\n\n##########\n# RADOS\n##########\nrados mkpool data # \u521b\u5efa Pool\necho Test-data > testfile.txt\nrados put tf.txt testfile.txt -p data # \u653e\u5165\u6587\u4ef6\nrados -p data ls # \u67e5\u770b\u5bf9\u8c61\nceph osd map data tf.txt # \u627e\u5230\u5b58\u50a8\u7684\u5bf9\u8c61\u6620\u5c04\n\nrados df # \u67e5\u770b\u4f7f\u7528\u91cf\n\nrados lspools # \u67e5\u770b\u6240\u6709\u7684 rados pool\nrados -p metadata ls # \u67e5\u770b\u67d0\u4e2a pool \u4e2d\u7684\u5bf9\u8c61\n\n# \u83b7\u53d6 pg pgp \u6570\u91cf\nceph osd pool get data pgp_num\nceph osd pool get data pg_num\n# \u67e5\u770b\u526f\u672c\u6570\u91cf\nceph osd dump | grep size\n\n\n##########\n# RDB\n##########\n# \u6302\u8f7d RBD\n# \u5c3d\u91cf\u4f7f\u7528 3.5+ \u5185\u6838\u7248\u672c\nuname -r\nmodprobe rbd\n\nrbd create rbd-1\nrbd info rbd-1\nrbd map rbd/rbd-1\n# \u67e5\u770b\u6620\u5c04\u7684\u8bbe\u5907\nrbd showmapped\n# \u6302\u8f7d\u6587\u4ef6\u7cfb\u7edf\nmkfs.btrfs /dev/rbd0\nmkdir -p /mnt/rbd/rbd-1\nmount /dev/rbd0 /mnt/rbd/rbd-1\n# \u7136\u540e\u5c31\u53ef\u4ee5\u4f7f\u7528\u4e86\n# docker run --name redis -v /mnt/rbd/rbd-1:/data -d redis redis-server --appendonly yes\n# docker exec -it redis redis-cli set a 1\n# docker exec -it redis redis-cli keys *\n# cat /mnt/rbd/rbd-1/appendonly.aof\n\n# \u6620\u5c04\u53ef\u80fd\u4f1a\u51fa\u73b0\u7531\u4e8e krbd \u4e0d\u652f\u6301 ceph rbd \u7279\u6027\u7684\u9519\u8bef\n# \u67e5\u770b Ceph \u6253\u5f00\u7684\u7279\u6027\nceph --show-config|grep rbd|grep features\n# layering      | 1\n# striping      | 2\n# exclusive-lock| 3\n# object-map    | 8\n# fast-diff     | 16\n# deep-flatten  | 32\n\n# \u5173\u95ed\u76f8\u5173\u7279\u6027\nrbd feature disable rbd/rbd-1 deep-flatten\nrbd feature disable rbd/rbd-1 fast-diff\nrbd feature disable rbd/rbd-1 object-map\nrbd feature disable rbd/rbd-1 exclusive-lock\n# \u4e5f\u53ef\u4ee5\u4f7f\u7528\n# for i in deep-flatten fast-diff object-map exclusive-lock; do rbd feature disable rbd/rbd-1 $i; done\n\n# \u53ef\u4ee5\u5728\u914d\u7f6e\u4e2d\u6dfb\u52a0 rbd_default_features = 3 \u4ee5\u4fee\u6539\u521b\u5efa\u51fa\u7684 rbd \u7279\u6027\n# \u5230 4.6 \u4e3a\u6b62,\u4f9d\u7136\u53ea\u652f\u6301 1+2\n\n# CephFS\nceph-deploy mds create ceph-node1\n\nceph osd pool create fs_data 128 128\nceph osd pool create fs_metadata 128 128\n\nceph fs new cephfs fs_data fs_metadata\nceph fs ls\nceph mds stat\n\nmkdir -p /mnt/cfs\n# \u67e5\u770b secret \u503c\ncat ceph.client.admin.keyring\nmount -t ceph \u5730\u5740:6789:/ /mnt/cfs -o name=admin,secret=\u4e0a\u9762\u7684secret\u503c\n# \u4e5f\u53ef\u4ee5\u5c06 secret \u653e\u5728\u6587\u4ef6\u4e2d\n# mount -t ceph \u5730\u5740:6789:/ /mnt/cfs -o name=admin,secretfile=\u6587\u4ef6\u8def\u5f84\n\n")),Object(r.b)("h2",{id:"troubleshooting"},"Troubleshooting"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"\u65e5\u5fd7\u76ee\u5f55 ",Object(r.b)("inlineCode",{parentName:"li"},"/var/log/ceph")),Object(r.b)("li",{parentName:"ul"},"Admin Socket ",Object(r.b)("inlineCode",{parentName:"li"},"/var/run/ceph"),Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"\u4f7f\u7528\u4e0a\u8ff0\u7684 socket \u6765\u64cd\u4f5c"),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"ceph daemon mon.node-2 help")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("inlineCode",{parentName:"li"},"ceph daemon /var/run/ceph/ceph-mon.hd2-2.asok help")))),Object(r.b)("li",{parentName:"ul"},"\u67e5\u770b systemd \u64cd\u4f5c\u65e5\u5fd7 ",Object(r.b)("inlineCode",{parentName:"li"},"journalctl -xe")),Object(r.b)("li",{parentName:"ul"},"\u914d\u7f6e\u6570\u636e\u5b58\u50a8\u76ee\u5f55 ",Object(r.b)("inlineCode",{parentName:"li"},"/var/lib/ceph/"))),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{}),"")),Object(r.b)("p",null,Object(r.b)("a",Object(a.a)({parentName:"p"},{href:"http://docs.ceph.com/docs/jewel/rados/troubleshooting/troubleshooting-osd/"}),"http://docs.ceph.com/docs/jewel/rados/troubleshooting/troubleshooting-osd/")),Object(r.b)("h2",{id:"\u6280\u672f\u6bd4\u8f83"},"\u6280\u672f\u6bd4\u8f83"),Object(r.b)("h3",{id:"vs-hdfs"},"vs HDFS"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"HDFS",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"Name Node \u4f1a\u9020\u6210\u5355\u70b9,\u4e14\u662f\u6574\u4e2a\u7cfb\u7edf\u4e2d\u7684\u74f6\u9888"))),Object(r.b)("li",{parentName:"ul"},"Ceph",Object(r.b)("ul",{parentName:"li"},Object(r.b)("li",{parentName:"ul"},"\u96c6\u7fa4\u4e3a AA \u6a21\u5f0f, \u65e0\u5355\u70b9"),Object(r.b)("li",{parentName:"ul"},"CRUSH \u4f7f\u5f97\u5bf9\u8c61\u5b58\u50a8\u4e0d\u4f1a\u5b58\u5728\u4e2d\u5fc3\u670d\u52a1,\u67b6\u6784\u4e2d\u65e0\u74f6\u9888")))),Object(r.b)("h2",{id:"\u547d\u4ee4\u884c"},"\u547d\u4ee4\u884c"),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{}),"$ rbd create -h\nusage: rbd <command> ...\n\nCommand-line interface for managing Ceph RBD images.\n\nPositional arguments:\n  <command>\n    bench-write                 Simple write benchmark.\n    children                    Display children of snapshot.\n    clone                       Clone a snapshot into a COW child image.\n    copy (cp)                   Copy src image to dest.\n    create                      Create an empty image.\n    diff                        Print extents that differ since a previous\n                                snap, or image creation.\n    disk-usage (du)             Show disk usage stats for pool, image or\n                                snapshot\n    export                      Export image to file.\n    export-diff                 Export incremental diff to file.\n    feature disable             Disable the specified image feature.\n    feature enable              Enable the specified image feature.\n    flatten                     Fill clone with parent data (make it\n                                independent).\n    image-meta get              Image metadata get the value associated with\n                                the key.\n    image-meta list             Image metadata list keys with values.\n    image-meta remove           Image metadata remove the key and value\n                                associated.\n    image-meta set              Image metadata set key with value.\n    import                      Import image from file.\n    import-diff                 Import an incremental diff.\n    info                        Show information about image size, striping,\n                                etc.\n    journal export              Export image journal.\n    journal import              Import image journal.\n    journal info                Show information about image journal.\n    journal inspect             Inspect image journal for structural errors.\n    journal reset               Reset image journal.\n    journal status              Show status of image journal.\n    list (ls)                   List rbd images.\n    lock add                    Take a lock on an image.\n    lock list (lock ls)         Show locks held on an image.\n    lock remove (lock rm)       Release a lock on an image.\n    map                         Map image to a block device using the kernel.\n    merge-diff                  Merge two diff exports together.\n    mirror image demote         Demote an image to non-primary for RBD\n                                mirroring.\n    mirror image disable        Disable RBD mirroring for an image.\n    mirror image enable         Enable RBD mirroring for an image.\n    mirror image promote        Promote an image to primary for RBD mirroring.\n    mirror image resync         Force resync to primary image for RBD mirroring.\n    mirror image status         Show RDB mirroring status for an image.\n    mirror pool disable         Disable RBD mirroring by default within a pool.\n    mirror pool enable          Enable RBD mirroring by default within a pool.\n    mirror pool info            Show information about the pool mirroring\n                                configuration.\n    mirror pool peer add        Add a mirroring peer to a pool.\n    mirror pool peer remove     Remove a mirroring peer from a pool.\n    mirror pool peer set        Update mirroring peer settings.\n    mirror pool status          Show status for all mirrored images in the pool.\n    nbd list (nbd ls)           List the nbd devices already used.\n    nbd map                     Map image to a nbd device.\n    nbd unmap                   Unmap a nbd device.\n    object-map rebuild          Rebuild an invalid object map.\n    remove (rm)                 Delete an image.\n    rename (mv)                 Rename image within pool.\n    resize                      Resize (expand or shrink) image.\n    showmapped                  Show the rbd images mapped by the kernel.\n    snap create (snap add)      Create a snapshot.\n    snap list (snap ls)         Dump list of image snapshots.\n    snap protect                Prevent a snapshot from being deleted.\n    snap purge                  Deletes all snapshots.\n    snap remove (snap rm)       Deletes a snapshot.\n    snap rename                 Rename a snapshot.\n    snap rollback (snap revert) Rollback image to snapshot.\n    snap unprotect              Allow a snapshot to be deleted.\n    status                      Show the status of this image.\n    unmap                       Unmap a rbd device that was used by the kernel.\n    watch                       Watch events on image.\n\nOptional arguments:\n  -c [ --conf ] arg     path to cluster configuration\n  --cluster arg         cluster name\n  --id arg              client id (without 'client.' prefix)\n  --user arg            client id (without 'client.' prefix)\n  -n [ --name ] arg     client name\n  -m [ --mon_host ] arg monitor host\n  --secret arg          path to secret key (deprecated)\n  -K [ --keyfile ] arg  path to secret key\n  -k [ --keyring ] arg  path to keyring\n\nSee 'rbd help <command>' for help on a specific command.\n")),Object(r.b)("pre",null,Object(r.b)("code",Object(a.a)({parentName:"pre"},{}),"$ ceph -h\n\n General usage:\n ==============\nusage: ceph [-h] [-c CEPHCONF] [-i INPUT_FILE] [-o OUTPUT_FILE]\n            [--id CLIENT_ID] [--name CLIENT_NAME] [--cluster CLUSTER]\n            [--admin-daemon ADMIN_SOCKET] [--admin-socket ADMIN_SOCKET_NOPE]\n            [-s] [-w] [--watch-debug] [--watch-info] [--watch-sec]\n            [--watch-warn] [--watch-error] [--version] [--verbose] [--concise]\n            [-f {json,json-pretty,xml,xml-pretty,plain}]\n            [--connect-timeout CLUSTER_TIMEOUT]\n\nCeph administration tool\n\noptional arguments:\n  -h, --help            request mon help\n  -c CEPHCONF, --conf CEPHCONF\n                        ceph configuration file\n  -i INPUT_FILE, --in-file INPUT_FILE\n                        input file\n  -o OUTPUT_FILE, --out-file OUTPUT_FILE\n                        output file\n  --id CLIENT_ID, --user CLIENT_ID\n                        client id for authentication\n  --name CLIENT_NAME, -n CLIENT_NAME\n                        client name for authentication\n  --cluster CLUSTER     cluster name\n  --admin-daemon ADMIN_SOCKET\n                        submit admin-socket commands (\"help\" for help\n  --admin-socket ADMIN_SOCKET_NOPE\n                        you probably mean --admin-daemon\n  -s, --status          show cluster status\n  -w, --watch           watch live cluster changes\n  --watch-debug         watch debug events\n  --watch-info          watch info events\n  --watch-sec           watch security events\n  --watch-warn          watch warn events\n  --watch-error         watch error events\n  --version, -v         display version\n  --verbose             make verbose\n  --concise             make less verbose\n  -f {json,json-pretty,xml,xml-pretty,plain}, --format {json,json-pretty,xml,xml-pretty,plain}\n  --connect-timeout CLUSTER_TIMEOUT\n                        set a timeout for connecting to the cluster\n\n Monitor commands:\n =================\n[Contacting monitor, timeout after 5 seconds]\nauth add <entity> {<caps> [<caps>...]}   add auth info for <entity> from input\n                                          file, or random key if no input is\n                                          given, and/or any caps specified in\n                                          the command\nauth caps <entity> <caps> [<caps>...]    update caps for <name> from caps\n                                          specified in the command\nauth del <entity>                        delete all caps for <name>\nauth export {<entity>}                   write keyring for requested entity, or\n                                          master keyring if none given\nauth get <entity>                        write keyring file with requested key\nauth get-key <entity>                    display requested key\nauth get-or-create <entity> {<caps>      add auth info for <entity> from input\n [<caps>...]}                             file, or random key if no input given,\n                                          and/or any caps specified in the\n                                          command\nauth get-or-create-key <entity> {<caps>  get, or add, key for <name> from\n [<caps>...]}                             system/caps pairs specified in the\n                                          command.  If key already exists, any\n                                          given caps must match the existing\n                                          caps for that key.\nauth import                              auth import: read keyring file from -i\n                                          <file>\nauth list                                list authentication state\nauth print-key <entity>                  display requested key\nauth print_key <entity>                  display requested key\nauth rm <entity>                         remove all caps for <name>\ncompact                                  cause compaction of monitor's leveldb\n                                          storage (DEPRECATED)\nconfig-key del <key>                     delete <key>\nconfig-key exists <key>                  check for <key>'s existence\nconfig-key get <key>                     get <key>\nconfig-key list                          list keys\nconfig-key put <key> {<val>}             put <key>, value <val>\nconfig-key rm <key>                      rm <key>\ndf {detail}                              show cluster free space stats\nfs add_data_pool <fs_name> <pool>        add data pool <pool>\nfs dump {<int[0-]>}                      dump all CephFS status, optionally\n                                          from epoch\nfs flag set enable_multiple <val> {--    Set a global CephFS flag\n yes-i-really-mean-it}\nfs get <fs_name>                         get info about one filesystem\nfs ls                                    list filesystems\nfs new <fs_name> <metadata> <data>       make new filesystem using named pools\n                                          <metadata> and <data>\nfs reset <fs_name> {--yes-i-really-mean- disaster recovery only: reset to a\n it}                                      single-MDS map\nfs rm <fs_name> {--yes-i-really-mean-it} disable the named filesystem\nfs rm_data_pool <fs_name> <pool>         remove data pool <pool>\nfs set <fs_name> max_mds|max_file_size|  set mds parameter <var> to <val>\n allow_new_snaps|inline_data|cluster_\n down|allow_multimds|allow_dirfrags\n <val> {<confirm>}\nfs set_default <fs_name>                 set the default to the named filesystem\nfsid                                     show cluster FSID/UUID\nhealth {detail}                          show cluster health\nheap dump|start_profiler|stop_profiler|  show heap usage info (available only\n release|stats                            if compiled with tcmalloc)\ninjectargs <injected_args> [<injected_   inject config arguments into monitor\n args>...]\nlog <logtext> [<logtext>...]             log supplied text to the monitor log\nmds add_data_pool <pool>                 add data pool <pool>\nmds cluster_down                         take MDS cluster down\nmds cluster_up                           bring MDS cluster up\nmds compat rm_compat <int[0-]>           remove compatible feature\nmds compat rm_incompat <int[0-]>         remove incompatible feature\nmds compat show                          show mds compatibility settings\nmds deactivate <who>                     stop mds\nmds dump {<int[0-]>}                     dump legacy MDS cluster info,\n                                          optionally from epoch\nmds fail <who>                           force mds to status failed\nmds getmap {<int[0-]>}                   get MDS map, optionally from epoch\nmds metadata <who>                       fetch metadata for mds <who>\nmds newfs <int[0-]> <int[0-]> {--yes-i-  make new filesystem using pools\n really-mean-it}                          <metadata> and <data>\nmds remove_data_pool <pool>              remove data pool <pool>\nmds repaired <rank>                      mark a damaged MDS rank as no longer\n                                          damaged\nmds rm <int[0-]>                         remove nonactive mds\nmds rm_data_pool <pool>                  remove data pool <pool>\nmds rmfailed <who> {<confirm>}           remove failed mds\nmds set max_mds|max_file_size|allow_new_ set mds parameter <var> to <val>\n snaps|inline_data|allow_multimds|allow_\n dirfrags <val> {<confirm>}\nmds set_max_mds <int[0-]>                set max MDS index\nmds set_state <int[0-]> <int[0-20]>      set mds state of <gid> to <numeric-\n                                          state>\nmds stat                                 show MDS status\nmds stop <who>                           stop mds\nmds tell <who> <args> [<args>...]        send command to particular mds\nmon add <name> <IPaddr[:port]>           add new monitor named <name> at <addr>\nmon compact                              cause compaction of monitor's leveldb\n                                          storage\nmon dump {<int[0-]>}                     dump formatted monmap (optionally from\n                                          epoch)\nmon getmap {<int[0-]>}                   get monmap\nmon metadata <id>                        fetch metadata for mon <id>\nmon remove <name>                        remove monitor named <name>\nmon rm <name>                            remove monitor named <name>\nmon scrub                                scrub the monitor stores\nmon stat                                 summarize monitor status\nmon sync force {--yes-i-really-mean-it}  force sync of and clear monitor store\n {--i-know-what-i-am-doing}\nmon_status                               report status of monitors\nnode ls {all|osd|mon|mds}                list all nodes in cluster [type]\nosd blacklist add|rm <EntityAddr>        add (optionally until <expire> seconds\n {<float[0.0-]>}                          from now) or remove <addr> from\n                                          blacklist\nosd blacklist clear                      clear all blacklisted clients\nosd blacklist ls                         show blacklisted clients\nosd blocked-by                           print histogram of which OSDs are\n                                          blocking their peers\nosd create {<uuid>} {<int[0-]>}          create new osd (with optional UUID and\n                                          ID)\nosd crush add <osdname (id|osd.id)>      add or update crushmap position and\n <float[0.0-]> <args> [<args>...]         weight for <name> with <weight> and\n                                          location <args>\nosd crush add-bucket <name> <type>       add no-parent (probably root) crush\n                                          bucket <name> of type <type>\nosd crush create-or-move <osdname (id|   create entry or move existing entry\n osd.id)> <float[0.0-]> <args> [<args>..  for <name> <weight> at/to location\n .]                                       <args>\nosd crush dump                           dump crush map\nosd crush get-tunable straw_calc_version get crush tunable <tunable>\nosd crush link <name> <args> [<args>...] link existing entry for <name> under\n                                          location <args>\nosd crush move <name> <args> [<args>...] move existing entry for <name> to\n                                          location <args>\nosd crush remove <name> {<ancestor>}     remove <name> from crush map (\n                                          everywhere, or just at <ancestor>)\nosd crush rename-bucket <srcname>        rename bucket <srcname> to <dstname>\n <dstname>\nosd crush reweight <name> <float[0.0-]>  change <name>'s weight to <weight> in\n                                          crush map\nosd crush reweight-all                   recalculate the weights for the tree\n                                          to ensure they sum correctly\nosd crush reweight-subtree <name>        change all leaf items beneath <name>\n <float[0.0-]>                            to <weight> in crush map\nosd crush rm <name> {<ancestor>}         remove <name> from crush map (\n                                          everywhere, or just at <ancestor>)\nosd crush rule create-erasure <name>     create crush rule <name> for erasure\n {<profile>}                              coded pool created with <profile> (\n                                          default default)\nosd crush rule create-simple <name>      create crush rule <name> to start from\n <root> <type> {firstn|indep}             <root>, replicate across buckets of\n                                          type <type>, using a choose mode of\n                                          <firstn|indep> (default firstn; indep\n                                          best for erasure pools)\nosd crush rule dump {<name>}             dump crush rule <name> (default all)\nosd crush rule list                      list crush rules\nosd crush rule ls                        list crush rules\nosd crush rule rm <name>                 remove crush rule <name>\nosd crush set                            set crush map from input file\nosd crush set <osdname (id|osd.id)>      update crushmap position and weight\n <float[0.0-]> <args> [<args>...]         for <name> to <weight> with location\n                                          <args>\nosd crush set-tunable straw_calc_        set crush tunable <tunable> to <value>\n version <int>\nosd crush show-tunables                  show current crush tunables\nosd crush tree                           dump crush buckets and items in a tree\n                                          view\nosd crush tunables legacy|argonaut|      set crush tunables values to <profile>\n bobtail|firefly|hammer|jewel|optimal|\n default\nosd crush unlink <name> {<ancestor>}     unlink <name> from crush map (\n                                          everywhere, or just at <ancestor>)\nosd deep-scrub <who>                     initiate deep scrub on osd <who>\nosd df {plain|tree}                      show OSD utilization\nosd down <ids> [<ids>...]                set osd(s) <id> [<id>...] down\nosd dump {<int[0-]>}                     print summary of OSD map\nosd erasure-code-profile get <name>      get erasure code profile <name>\nosd erasure-code-profile ls              list all erasure code profiles\nosd erasure-code-profile rm <name>       remove erasure code profile <name>\nosd erasure-code-profile set <name>      create erasure code profile <name>\n {<profile> [<profile>...]}               with [<key[=value]> ...] pairs. Add a\n                                          --force at the end to override an\n                                          existing profile (VERY DANGEROUS)\nosd find <int[0-]>                       find osd <id> in the CRUSH map and\n                                          show its location\nosd getcrushmap {<int[0-]>}              get CRUSH map\nosd getmap {<int[0-]>}                   get OSD map\nosd getmaxosd                            show largest OSD id\nosd in <ids> [<ids>...]                  set osd(s) <id> [<id>...] in\nosd lost <int[0-]> {--yes-i-really-mean- mark osd as permanently lost. THIS\n it}                                      DESTROYS DATA IF NO MORE REPLICAS\n                                          EXIST, BE CAREFUL\nosd ls {<int[0-]>}                       show all OSD ids\nosd lspools {<int>}                      list pools\nosd map <poolname> <objectname>          find pg for <object> in <pool> with\n {<nspace>}                               [namespace]\nosd metadata {<int[0-]>}                 fetch metadata for osd {id} (default\n                                          all)\nosd out <ids> [<ids>...]                 set osd(s) <id> [<id>...] out\nosd pause                                pause osd\nosd perf                                 print dump of OSD perf summary stats\nosd pg-temp <pgid> {<id> [<id>...]}      set pg_temp mapping pgid:[<id> [<id>...\n                                          ]] (developers only)\nosd pool create <poolname> <int[0-]>     create pool\n {<int[0-]>} {replicated|erasure}\n {<erasure_code_profile>} {<ruleset>}\n {<int>}\nosd pool delete <poolname> {<poolname>}  delete pool\n {--yes-i-really-really-mean-it}\nosd pool get <poolname> size|min_size|   get pool parameter <var>\n crash_replay_interval|pg_num|pgp_num|\n crush_ruleset|hashpspool|nodelete|\n nopgchange|nosizechange|write_fadvise_\n dontneed|noscrub|nodeep-scrub|hit_set_\n type|hit_set_period|hit_set_count|hit_\n set_fpp|auid|target_max_objects|target_\n max_bytes|cache_target_dirty_ratio|\n cache_target_dirty_high_ratio|cache_\n target_full_ratio|cache_min_flush_age|\n cache_min_evict_age|erasure_code_\n profile|min_read_recency_for_promote|\n all|min_write_recency_for_promote|fast_\n read|hit_set_grade_decay_rate|hit_set_\n search_last_n|scrub_min_interval|scrub_\n max_interval|deep_scrub_interval|\n recovery_priority|recovery_op_priority|\n scrub_priority\nosd pool get-quota <poolname>            obtain object or byte limits for pool\nosd pool ls {detail}                     list pools\nosd pool mksnap <poolname> <snap>        make snapshot <snap> in <pool>\nosd pool rename <poolname> <poolname>    rename <srcpool> to <destpool>\nosd pool rm <poolname> {<poolname>} {--  remove pool\n yes-i-really-really-mean-it}\nosd pool rmsnap <poolname> <snap>        remove snapshot <snap> from <pool>\nosd pool set <poolname> size|min_size|   set pool parameter <var> to <val>\n crash_replay_interval|pg_num|pgp_num|\n crush_ruleset|hashpspool|nodelete|\n nopgchange|nosizechange|write_fadvise_\n dontneed|noscrub|nodeep-scrub|hit_set_\n type|hit_set_period|hit_set_count|hit_\n set_fpp|use_gmt_hitset|debug_fake_ec_\n pool|target_max_bytes|target_max_\n objects|cache_target_dirty_ratio|cache_\n target_dirty_high_ratio|cache_target_\n full_ratio|cache_min_flush_age|cache_\n min_evict_age|auid|min_read_recency_\n for_promote|min_write_recency_for_\n promote|fast_read|hit_set_grade_decay_\n rate|hit_set_search_last_n|scrub_min_\n interval|scrub_max_interval|deep_scrub_\n interval|recovery_priority|recovery_op_\n priority|scrub_priority <val> {--yes-i-\n really-mean-it}\nosd pool set-quota <poolname> max_       set object or byte limit on pool\n objects|max_bytes <val>\nosd pool stats {<name>}                  obtain stats from all pools, or from\n                                          specified pool\nosd primary-affinity <osdname (id|osd.   adjust osd primary-affinity from 0.0 <=\n id)> <float[0.0-1.0]>                     <weight> <= 1.0\nosd primary-temp <pgid> <id>             set primary_temp mapping pgid:<id>|-1 (\n                                          developers only)\nosd repair <who>                         initiate repair on osd <who>\nosd reweight <int[0-]> <float[0.0-1.0]>  reweight osd to 0.0 < <weight> < 1.0\nosd reweight-by-pg {<int>} {<float>}     reweight OSDs by PG distribution\n {<int>} {<poolname> [<poolname>...]}     [overload-percentage-for-\n                                          consideration, default 120]\nosd reweight-by-utilization {<int>}      reweight OSDs by utilization [overload-\n {<float>} {<int>} {--no-increasing}      percentage-for-consideration, default\n                                          120]\nosd rm <ids> [<ids>...]                  remove osd(s) <id> [<id>...] in\nosd scrub <who>                          initiate scrub on osd <who>\nosd set full|pause|noup|nodown|noout|    set <key>\n noin|nobackfill|norebalance|norecover|\n noscrub|nodeep-scrub|notieragent|\n sortbitwise\nosd setcrushmap                          set crush map from input file\nosd setmaxosd <int[0-]>                  set new maximum osd value\nosd stat                                 print summary of OSD map\nosd test-reweight-by-pg {<int>}          dry run of reweight OSDs by PG\n {<float>} {<int>} {<poolname>            distribution [overload-percentage-for-\n [<poolname>...]}                         consideration, default 120]\nosd test-reweight-by-utilization         dry run of reweight OSDs by\n {<int>} {<float>} {<int>} {--no-         utilization [overload-percentage-for-\n increasing}                              consideration, default 120]\nosd thrash <int[0-]>                     thrash OSDs for <num_epochs>\nosd tier add <poolname> <poolname> {--   add the tier <tierpool> (the second\n force-nonempty}                          one) to base pool <pool> (the first\n                                          one)\nosd tier add-cache <poolname>            add a cache <tierpool> (the second one)\n <poolname> <int[0-]>                     of size <size> to existing pool\n                                          <pool> (the first one)\nosd tier cache-mode <poolname> none|     specify the caching mode for cache\n writeback|forward|readonly|readforward|  tier <pool>\n proxy|readproxy {--yes-i-really-mean-\n it}\nosd tier remove <poolname> <poolname>    remove the tier <tierpool> (the second\n                                          one) from base pool <pool> (the first\n                                          one)\nosd tier remove-overlay <poolname>       remove the overlay pool for base pool\n                                          <pool>\nosd tier rm <poolname> <poolname>        remove the tier <tierpool> (the second\n                                          one) from base pool <pool> (the first\n                                          one)\nosd tier rm-overlay <poolname>           remove the overlay pool for base pool\n                                          <pool>\nosd tier set-overlay <poolname>          set the overlay pool for base pool\n <poolname>                               <pool> to be <overlaypool>\nosd tree {<int[0-]>}                     print OSD tree\nosd unpause                              unpause osd\nosd unset full|pause|noup|nodown|noout|  unset <key>\n noin|nobackfill|norebalance|norecover|\n noscrub|nodeep-scrub|notieragent|\n sortbitwise\nosd utilization                          get basic pg distribution stats\npg debug unfound_objects_exist|degraded_ show debug info about pgs\n pgs_exist\npg deep-scrub <pgid>                     start deep-scrub on <pgid>\npg dump {all|summary|sum|delta|pools|    show human-readable versions of pg map\n osds|pgs|pgs_brief [all|summary|sum|     (only 'all' valid with plain)\n delta|pools|osds|pgs|pgs_brief...]}\npg dump_json {all|summary|sum|pools|     show human-readable version of pg map\n osds|pgs [all|summary|sum|pools|osds|    in json only\n pgs...]}\npg dump_pools_json                       show pg pools info in json only\npg dump_stuck {inactive|unclean|stale|   show information about stuck pgs\n undersized|degraded [inactive|unclean|\n stale|undersized|degraded...]} {<int>}\npg force_create_pg <pgid>                force creation of pg <pgid>\npg getmap                                get binary pg map to -o/stdout\npg ls {<int>} {active|clean|down|replay| list pg with specific pool, osd, state\n splitting|scrubbing|scrubq|degraded|\n inconsistent|peering|repair|recovering|\n backfill_wait|incomplete|stale|\n remapped|deep_scrub|backfill|backfill_\n toofull|recovery_wait|undersized|\n activating|peered [active|clean|down|\n replay|splitting|scrubbing|scrubq|\n degraded|inconsistent|peering|repair|\n recovering|backfill_wait|incomplete|\n stale|remapped|deep_scrub|backfill|\n backfill_toofull|recovery_wait|\n undersized|activating|peered...]}\npg ls-by-osd <osdname (id|osd.id)>       list pg on osd [osd]\n {<int>} {active|clean|down|replay|\n splitting|scrubbing|scrubq|degraded|\n inconsistent|peering|repair|recovering|\n backfill_wait|incomplete|stale|\n remapped|deep_scrub|backfill|backfill_\n toofull|recovery_wait|undersized|\n activating|peered [active|clean|down|\n replay|splitting|scrubbing|scrubq|\n degraded|inconsistent|peering|repair|\n recovering|backfill_wait|incomplete|\n stale|remapped|deep_scrub|backfill|\n backfill_toofull|recovery_wait|\n undersized|activating|peered...]}\npg ls-by-pool <poolstr> {active|clean|   list pg with pool = [poolname | poolid]\n down|replay|splitting|scrubbing|scrubq|\n degraded|inconsistent|peering|repair|\n recovering|backfill_wait|incomplete|\n stale|remapped|deep_scrub|backfill|\n backfill_toofull|recovery_wait|\n undersized|activating|peered [active|\n clean|down|replay|splitting|scrubbing|\n scrubq|degraded|inconsistent|peering|\n repair|recovering|backfill_wait|\n incomplete|stale|remapped|deep_scrub|\n backfill|backfill_toofull|recovery_\n wait|undersized|activating|peered...]}\npg ls-by-primary <osdname (id|osd.id)>   list pg with primary = [osd]\n {<int>} {active|clean|down|replay|\n splitting|scrubbing|scrubq|degraded|\n inconsistent|peering|repair|recovering|\n backfill_wait|incomplete|stale|\n remapped|deep_scrub|backfill|backfill_\n toofull|recovery_wait|undersized|\n activating|peered [active|clean|down|\n replay|splitting|scrubbing|scrubq|\n degraded|inconsistent|peering|repair|\n recovering|backfill_wait|incomplete|\n stale|remapped|deep_scrub|backfill|\n backfill_toofull|recovery_wait|\n undersized|activating|peered...]}\npg map <pgid>                            show mapping of pg to osds\npg repair <pgid>                         start repair on <pgid>\npg scrub <pgid>                          start scrub on <pgid>\npg send_pg_creates                       trigger pg creates to be issued\npg set_full_ratio <float[0.0-1.0]>       set ratio at which pgs are considered\n                                          full\npg set_nearfull_ratio <float[0.0-1.0]>   set ratio at which pgs are considered\n                                          nearly full\npg stat                                  show placement group status.\nquorum enter|exit                        enter or exit quorum\nquorum_status                            report status of monitor quorum\nreport {<tags> [<tags>...]}              report full status of cluster,\n                                          optional title tag strings\nscrub                                    scrub the monitor stores (DEPRECATED)\nstatus                                   show cluster status\nsync force {--yes-i-really-mean-it} {--  force sync of and clear monitor store (\n i-know-what-i-am-doing}                  DEPRECATED)\ntell <name (type.id)> <args> [<args>...] send a command to a specific daemon\nversion                                  show mon daemon version\n")),Object(r.b)("h2",{id:"faq"},"FAQ"),Object(r.b)("h3",{id:"health_err-64-pgs-are-stuck-inactive-for-more-than-300-seconds-64-pgs-stuck-inactive"},"HEALTH_ERR 64 pgs are stuck inactive for more than 300 seconds; 64 pgs stuck inactive"),Object(r.b)("p",null,"\u8fd9\u4e2a\u662f\u6709\u53ef\u80fd\u7531\u5f88\u591a\u95ee\u9898\u5f15\u8d77\u7684,\u53ef\u5206\u522b\u67e5\u770b\u5176\u5b83\u670d\u52a1\u4e0a\u7684\u65e5\u5fd7\u6765\u5224\u65ad ",Object(r.b)("inlineCode",{parentName:"p"},"cat /var/log/ceph/ceph*")),Object(r.b)("h3",{id:"error-osd-init-failed-36-file-name-too-long"},"ERROR: osd init failed: (36) File name too long"),Object(r.b)("p",null,"\u6587\u4ef6\u540d\u592a\u957f,\u53ef\u80fd\u662f\u7531\u4e8e\u6587\u4ef6\u7cfb\u7edf\u7c7b\u578b\u5bfc\u81f4\u7684 ",Object(r.b)("inlineCode",{parentName:"p"},"df -T"),",\u4e00\u822c\u5efa\u8bae\u4f7f\u7528 XFS \u6216 Btrfs, \u4f46\u5927\u591a\u6570\u60c5\u51b5\u4e0b\u662f ext4, \u6587\u4ef6\u540d\u4e0d\u80fd\u8d85\u8fc7 1024b."))}p.isMDXComponent=!0},561:function(e,t,n){"use strict";n.d(t,"a",(function(){return d})),n.d(t,"b",(function(){return u}));var a=n(0),o=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function s(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function l(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=o.a.createContext({}),p=function(e){var t=o.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):s(s({},t),e)),n},d=function(e){var t=p(e.components);return o.a.createElement(c.Provider,{value:t},e.children)},m={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},b=o.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,c=l(e,["components","mdxType","originalType","parentName"]),d=p(n),b=a,u=d["".concat(i,".").concat(b)]||d[b]||m[b]||r;return n?o.a.createElement(u,s(s({ref:t},c),{},{components:n})):o.a.createElement(u,s({ref:t},c))}));function u(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=b;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:a,i[1]=s;for(var c=2;c<r;c++)i[c]=n[c];return o.a.createElement.apply(null,i)}return o.a.createElement.apply(null,n)}b.displayName="MDXCreateElement"}}]);