"use strict";(self.webpackChunkwener_website=self.webpackChunkwener_website||[]).push([[90524],{49613:function(t,e,a){a.d(e,{Zo:function(){return u},kt:function(){return m}});var n=a(59496);function r(t,e,a){return e in t?Object.defineProperty(t,e,{value:a,enumerable:!0,configurable:!0,writable:!0}):t[e]=a,t}function l(t,e){var a=Object.keys(t);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(t);e&&(n=n.filter((function(e){return Object.getOwnPropertyDescriptor(t,e).enumerable}))),a.push.apply(a,n)}return a}function o(t){for(var e=1;e<arguments.length;e++){var a=null!=arguments[e]?arguments[e]:{};e%2?l(Object(a),!0).forEach((function(e){r(t,e,a[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(t,Object.getOwnPropertyDescriptors(a)):l(Object(a)).forEach((function(e){Object.defineProperty(t,e,Object.getOwnPropertyDescriptor(a,e))}))}return t}function i(t,e){if(null==t)return{};var a,n,r=function(t,e){if(null==t)return{};var a,n,r={},l=Object.keys(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||(r[a]=t[a]);return r}(t,e);if(Object.getOwnPropertySymbols){var l=Object.getOwnPropertySymbols(t);for(n=0;n<l.length;n++)a=l[n],e.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(t,a)&&(r[a]=t[a])}return r}var p=n.createContext({}),c=function(t){var e=n.useContext(p),a=e;return t&&(a="function"==typeof t?t(e):o(o({},e),t)),a},u=function(t){var e=c(t.components);return n.createElement(p.Provider,{value:e},t.children)},f="mdxType",d={inlineCode:"code",wrapper:function(t){var e=t.children;return n.createElement(n.Fragment,{},e)}},g=n.forwardRef((function(t,e){var a=t.components,r=t.mdxType,l=t.originalType,p=t.parentName,u=i(t,["components","mdxType","originalType","parentName"]),f=c(a),g=r,m=f["".concat(p,".").concat(g)]||f[g]||d[g]||l;return a?n.createElement(m,o(o({ref:e},u),{},{components:a})):n.createElement(m,o({ref:e},u))}));function m(t,e){var a=arguments,r=e&&e.mdxType;if("string"==typeof t||r){var l=a.length,o=new Array(l);o[0]=g;var i={};for(var p in e)hasOwnProperty.call(e,p)&&(i[p]=e[p]);i.originalType=t,i[f]="string"==typeof t?t:r,o[1]=i;for(var c=2;c<l;c++)o[c]=a[c];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}g.displayName="MDXCreateElement"},24139:function(t,e,a){a.r(e),a.d(e,{assets:function(){return h},contentTitle:function(){return k},default:function(){return y},frontMatter:function(){return m},metadata:function(){return s},toc:function(){return b}});var n=a(49613),r=Object.defineProperty,l=Object.defineProperties,o=Object.getOwnPropertyDescriptors,i=Object.getOwnPropertySymbols,p=Object.prototype.hasOwnProperty,c=Object.prototype.propertyIsEnumerable,u=(t,e,a)=>e in t?r(t,e,{enumerable:!0,configurable:!0,writable:!0,value:a}):t[e]=a,f=(t,e)=>{for(var a in e||(e={}))p.call(e,a)&&u(t,a,e[a]);if(i)for(var a of i(e))c.call(e,a)&&u(t,a,e[a]);return t},d=(t,e)=>l(t,o(e)),g=(t,e)=>{var a={};for(var n in t)p.call(t,n)&&e.indexOf(n)<0&&(a[n]=t[n]);if(null!=t&&i)for(var n of i(t))e.indexOf(n)<0&&c.call(t,n)&&(a[n]=t[n]);return a};const m={title:"LLaMa"},k="llama",s={unversionedId:"ai/llm/llama",id:"ai/llm/llama",title:"LLaMa",description:"- LLaMA-7B, 3.5GB, 6GB",source:"@site/../notes/ai/llm/llama.md",sourceDirName:"ai/llm",slug:"/ai/llm/llama",permalink:"/notes/ai/llm/llama",draft:!1,editUrl:"https://github.com/wenerme/wener/edit/master/notes/../notes/ai/llm/llama.md",tags:[],version:"current",lastUpdatedBy:"wener",lastUpdatedAt:1699940620,formattedLastUpdatedAt:"Nov 14, 2023",frontMatter:{title:"LLaMa"},sidebar:"docs",previous:{title:"Alpaca",permalink:"/notes/ai/llm/alpaca"},next:{title:"LLM FAQ",permalink:"/notes/ai/llm/faq"}},h={},b=[{value:"Changelog (MDY)",id:"changelog-mdy",level:2},{value:"4-bit GPU Model Requirements",id:"4-bit-gpu-model-requirements",level:2},{value:"4-bit CPU/llama.cpp RAM Requirements",id:"4-bit-cpullamacpp-ram-requirements",level:2},{value:"LLaMA 16-bit Weights",id:"llama-16-bit-weights",level:2},{value:"LLaMA 4-bit Weights",id:"llama-4-bit-weights",level:2},{value:"WizardLM 13B Uncensored (05/10/2023)",id:"wizardlm-13b-uncensored-05102023",level:2},{value:"BluemoonRP 13B (05/07/2023)",id:"bluemoonrp-13b-05072023",level:2},{value:"Vicuna 13B Cocktail (05/07/2023)",id:"vicuna-13b-cocktail-05072023",level:2},{value:"GPT4-x-AlpacaDente2-30B (05/05/2023)",id:"gpt4-x-alpacadente2-30b-05052023",level:2},{value:"Vicuna 13B Free v1.1 (05/01/2023)",id:"vicuna-13b-free-v11-05012023",level:2},{value:"Pygmalion/Metharme 7B (04/30/2023)",id:"pygmalionmetharme-7b-04302023",level:2},{value:"GPT4-X-Alpasta 30B (04/29/2023)",id:"gpt4-x-alpasta-30b-04292023",level:2},{value:"OpenAssistant LLaMa 30B SFT 6 (04/23/2023)",id:"openassistant-llama-30b-sft-6-04232023",level:2},{value:"SuperCOT (04/22/2023)",id:"supercot-04222023",level:2},{value:"Previous Model List",id:"previous-model-list",level:2},{value:"LLaMA quantized 4-bit weights (ggml q4_0)",id:"llama-quantized-4-bit-weights-ggml-q4_0",level:2},{value:"2023-03-31 torrent magnet",id:"2023-03-31-torrent-magnet",level:4},{value:"Alpaca quantized 4-bit weights (ggml q4_0)",id:"alpaca-quantized-4-bit-weights-ggml-q4_0",level:2},{value:"GPT4All 7B quantized 4-bit weights (ggml q4_0)",id:"gpt4all-7b-quantized-4-bit-weights-ggml-q4_0",level:2},{value:"2023-03-31 torrent magnet",id:"2023-03-31-torrent-magnet-1",level:4},{value:"GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)",id:"gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_0",level:2},{value:"2023-04-01 torrent magnet",id:"2023-04-01-torrent-magnet",level:4},{value:"GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)",id:"gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128",level:2},{value:"2023-04-01 torrent magnet",id:"2023-04-01-torrent-magnet-1",level:4},{value:"Vicuna 13B quantized 4-bit weights (ggml q4_0)",id:"vicuna-13b-quantized-4-bit-weights-ggml-q4_0",level:2},{value:"2023-04-03 torrent magnet",id:"2023-04-03-torrent-magnet",level:4},{value:"OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 &amp; q4_1)",id:"openassistant-llama-13b-quantized-4-bit-weights-ggml-q4_0--q4_1",level:2},{value:"2023-04-07 torrent magnet | HuggingFace Hub direct download",id:"2023-04-07-torrent-magnet--huggingface-hub-direct-download",level:4},{value:"LLaMA float16 weights",id:"llama-float16-weights",level:2},{value:"2023-03-26 torrent magnet | HuggingFace Hub direct downloads",id:"2023-03-26-torrent-magnet--huggingface-hub-direct-downloads",level:4},{value:"Vicuna 13B float16 weights",id:"vicuna-13b-float16-weights",level:2},{value:"2023-04-03 torrent magnet",id:"2023-04-03-torrent-magnet-1",level:4},{value:"LLaMA quantized 4-bit weights (GPTQ format without groupsize)",id:"llama-quantized-4-bit-weights-gptq-format-without-groupsize",level:2},{value:"2023-03-26 torrent magnet",id:"2023-03-26-torrent-magnet",level:4},{value:"LLaMA quantized 4-bit weights (GPTQ format with groupsize 128)",id:"llama-quantized-4-bit-weights-gptq-format-with-groupsize-128",level:2},{value:"2023-03-26 torrent magnet",id:"2023-03-26-torrent-magnet-1",level:4},{value:"Alpaca quantized 4-bit weights (GPTQ format with groupsize 128)",id:"alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128",level:2},{value:"Vicuna 13B quantized 4-bit &amp; 8-bit weights (GPTQ format with groupsize 128)",id:"vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128",level:2},{value:"2023-04-03 torrent magnet",id:"2023-04-03-torrent-magnet-2",level:5}],N={toc:b},v="wrapper";function y(t){var e=t,{components:a}=e,r=g(e,["components"]);return(0,n.kt)(v,d(f(f({},N),r),{components:a,mdxType:"MDXLayout"}),(0,n.kt)("h1",f({},{id:"llama"}),"llama"),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},"LLaMA-7B, 3.5GB, 6GB"),(0,n.kt)("li",{parentName:"ul"},"LLaMA-13B, 6.5GB, 10GB"),(0,n.kt)("li",{parentName:"ul"},"LLaMA-30B, 15.8GB, 20GB"),(0,n.kt)("li",{parentName:"ul"},"LLaMA-65B, 31.2GB, 40GB"),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://news.ycombinator.com/item?id=35107058"}),"https://news.ycombinator.com/item?id=35107058")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://github.com/ZrrSkywalker/LLaMA-Adapter"}),"https://github.com/ZrrSkywalker/LLaMA-Adapter")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://huggingface.co/blog/stackllama"}),"https://huggingface.co/blog/stackllama"))),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-bash"}),'# py for\napk add \\\n  gcc g++ python3 py3-pip musl-dev cmake make pkgconf build-base \\\n  git openssh-client binutils coreutils util-linux findutils sed grep tar wget curl neofetch \\\n  rust cargo python3-dev openssl-dev linux-headers\n\n# llama.cpp\n# =========\ngit clone https://github.com/ggerganov/llama.cpp.git\ncd llama.cpp\nmake -j\n\n./main -m ./models/7B/ggml-model-q4_0.bin -p "Building a website can be done in 10 simple steps:" -n 512\n./main -m ./models/7B/ggml-model-q4_0.bin --file prompts/alpaca.txt --instruct --ctx_size 2048 --keep -1\n\n./main -m ./models/ggml-alpaca-7b-q4.bin --color -f ./prompts/alpaca.txt -ins -b 256 --top_k 10000 --temp 0.2 --repeat_penalty 1 -t 7\n\n# https://github.com/ymcui/Chinese-LLaMA-Alpaca\n# =========\napk add rust cargo python3-dev openssl-dev cmake linux-headers\npip install git+https://github.com/huggingface/transformers\npip install sentencepiece\npip install torch --index-url https://download.pytorch.org/whl/cpu\npip install peft\n\ngit clone https://github.com/huggingface/transformers\n\n# musl pthread_attr_setaffinity_np\npython ./transformers/src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n  --input_dir /ml/models/LLaMA \\\n  --model_size 7B \\\n  --output_dir /ml/models/LLaMA-hf\n')),(0,n.kt)("p",null,(0,n.kt)("strong",{parentName:"p"},"ggml")),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://github.com/ggerganov/llama.cpp/pull/1305"}),"https://github.com/ggerganov/llama.cpp/pull/1305")),(0,n.kt)("li",{parentName:"ul"},"ggjt v3")),(0,n.kt)("h1",f({},{id:"faq"}),"FAQ"),(0,n.kt)("h1",f({},{id:"ref"}),"Ref"),(0,n.kt)("hr",null),(0,n.kt)("ul",null,(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://rentry.org/lmg_models"}),"https://rentry.org/lmg_models")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://rentry.org/lmg-resources"}),"https://rentry.org/lmg-resources")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://rentry.org/ayumi_erp_rating"}),"https://rentry.org/ayumi_erp_rating")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"https://rentry.co/ALLMRR"}),"https://rentry.co/ALLMRR")),(0,n.kt)("li",{parentName:"ul"},(0,n.kt)("a",f({parentName:"li"},{href:"http://ayumi.m8geil.de/"}),"http://ayumi.m8geil.de/")),(0,n.kt)("li",{parentName:"ul"},"ERP - erotic role playing - \u60c5\u8272\u89d2\u8272\u626e\u6f14")),(0,n.kt)("p",null,"#->/lmg/ Model Links and Torrents <-"),(0,n.kt)("p",null,"[TOC2]"),(0,n.kt)("h2",f({},{id:"changelog-mdy"}),"Changelog (MDY)"),(0,n.kt)("p",null,"[05-10-2023]"," - Added WizardLM 13B Uncensored\n","[05-07-2023]"," - Added Vicuna 13B Cocktail, bluemoonrp-13b & AlpacaDente2\n","[05-05-2023]"," - Added CPU quantization variation links\n","[05-02-2023]"," - Initial Rentry"),(0,n.kt)("h2",f({},{id:"4-bit-gpu-model-requirements"}),"4-bit GPU Model Requirements"),(0,n.kt)("p",null,"!!! note VRAM Required takes full context (2048) into account. You may be able to load the model on GPU's with slightly lower VRAM, but you will not be able to run at full context. If you do not have enough RAM to load model, it will load into swap. Groupsize models will increase VRAM usage, as will running a LoRA alongside the model."),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model Parameters"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"VRAM Required"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"GPU Examples"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"RAM to Load"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"8GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"RTX 1660, 2060, AMD 5700xt, RTX 3050, RTX 3060, RTX 3070"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"6 GB")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"12GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"AMD 6900xt, RTX 2060 12GB, 3060 12GB, 3080 12GB, A2000"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"12GB")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"24GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"RTX 3090, RTX 4090, A4500, A5000, 6000, Tesla V100"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"32GB")),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"42GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"A100 80GB, NVIDIA Quadro RTX 8000, Quadro RTX A6000"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"64GB")))),(0,n.kt)("h2",f({},{id:"4-bit-cpullamacpp-ram-requirements"}),"4-bit CPU/llama.cpp RAM Requirements"),(0,n.kt)("p",null,"!!! note 5bit to 8bit Quantized models are becoming more common, and will obviously require more RAM. Will update these with the numbers when I have them."),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"4-bit"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"5-bit"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"8-bit"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"3.9 GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null})),(0,n.kt)("td",f({parentName:"tr"},{align:null}))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7.8 GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null})),(0,n.kt)("td",f({parentName:"tr"},{align:null}))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"19.5 GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null})),(0,n.kt)("td",f({parentName:"tr"},{align:null}))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"38.5 GB"),(0,n.kt)("td",f({parentName:"tr"},{align:null})),(0,n.kt)("td",f({parentName:"tr"},{align:null}))))),(0,n.kt)("h1",f({},{id:"original-weights"}),"Original Weights"),(0,n.kt)("h2",f({},{id:"llama-16-bit-weights"}),"LLaMA 16-bit Weights"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"The original LLaMA weights converted to Transformers @ 16bit. A torrent is available as well, but it uses outdated configuration files that will need to be updated. Note that these aren't for general use, as the VRAM requirements are beyond consumer scope.\n\n>Filtering : None\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B 16bit"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"HF Format"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Neko-Institute-of-Science/LLaMA-7B-HF"}),"HuggingFace"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B 16bit"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"HF Format"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF"}),"HuggingFace"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B 16bit"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"HF Format"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Neko-Institute-of-Science/LLaMA-30B-HF"}),"HuggingFace"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"65B 16bit"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"HF Format"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Neko-Institute-of-Science/LLaMA-13B-HF"}),"HuggingFace"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"All the above"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"HF Format"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:8d634925911a03f787d9f68ac075a9b24281573a&dn=Safe-LLaMA-HF-v2%20(4-04-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"Torrent Magnet"))))),(0,n.kt)("h2",f({},{id:"llama-4-bit-weights"}),"LLaMA 4-bit Weights"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"The original LLaMA weights quantized to 4-bit. The GPU CUDA versions have outdated tokenizer and configuration files. It is recommended to either update them with [this](https://rentry.org/544p2) or use the [universal LLaMA tokenizer.](https://github.com/oobabooga/text-generation-webui/blob/main/docs/LLaMA-model.md#option-1-pre-converted-weights)\n\n>Filtering : None\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B, 13B, 30B, 65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:481dee5424b7024433504803a90efd32dae40fdf&dn=LLaMA-ggml-4bit_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce"}),"Torrent Magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B, 13B, 30B, 65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU CUDA (no groupsize)"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:e88abf1b84290b162f00d3a9d79fb4f8719c2053&dn=LLaMA-HF-4bit&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"Torrent Magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B, 13B, 30B, 65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU CUDA (128gs)"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:88f7d9d2460ffcaf78b21e83012de00939eacb65&dn=LLaMA-HF-4bit-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"Torrent Magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B, 13B, 30B, 65B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU Triton"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Neko-Institute-of-Science"}),"Neko Institute of Science HF page"))))),(0,n.kt)("h1",f({},{id:"modelsfinetunesloras"}),"Models/Finetunes/LoRA's"),(0,n.kt)("h2",f({},{id:"wizardlm-13b-uncensored-05102023"}),"WizardLM 13B Uncensored (05/10/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),'This is WizardLM trained with a subset of the dataset - responses that contained alignment / moralizing were removed. The intent is to train a WizardLM that doesn\'t have alignment built-in, so that alignment (of any sort) can be added separately with for example with a RLHF LoRA.\n\nNote that despite being an "uncensored" model, several tests have demonstrated that the model will still refuse to comply with certain requests.\n\n>Filtering : Light\n')),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/TehVenom/WizardLM-13B-Uncensored-Q5_1-GGML"}),"Q5"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/ausboss/WizardLM-13B-Uncensored-4bit-128g"}),"Q4 CUDA 128gs"))))),(0,n.kt)("h2",f({},{id:"bluemoonrp-13b-05072023"}),"BluemoonRP 13B (05/07/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"An RP/ERP focused finetune of LLaMA 13B finetuned on BluemoonRP logs. It is designed to simulate a 2-person RP session. Two versions are provided; a standard 13B with 2K context and an experimental 13B with 4K context. It has a non-standard format (LEAD/ASSOCIATE), so ensure that you read the model card and use the correct syntax.\n\n>Filtering : None\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU & CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/reeducator/bluemoonrp-13b"}),"https://huggingface.co/reeducator/bluemoonrp-13b"))))),(0,n.kt)("h2",f({},{id:"vicuna-13b-cocktail-05072023"}),"Vicuna 13B Cocktail (05/07/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),'Vicuna 1.1 13B finetune incorporating various datasets in addition to the unfiltered ShareGPT. This is an experiment attempting to enhance the creativity of the Vicuna 1.1, while also reducing censorship as much as possible. All datasets have been cleaned. Additionally, only the "instruct" portion of GPTeacher has been used. It has a non-standard format (USER/ASSOCIATE), so ensure that you read the model card and use the correct syntax.\n\n>Filtering : Light\n')),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU & CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/reeducator/vicuna-13b-cocktail"}),"https://huggingface.co/reeducator/vicuna-13b-cocktail"))))),(0,n.kt)("h2",f({},{id:"gpt4-x-alpacadente2-30b-05052023"}),"GPT4-x-AlpacaDente2-30B (05/05/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"ChanSung's Alpaca-LoRA-30B-elina merged with Open Assistant's second Finetune. Testing in progress.\n\n>Filtering : Medium\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Lumpen1/GPT4-x-AlpacaDente2-30b-ggml-q5_0"}),"Q5"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit"}),"Q4 CUDA"))))),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit"}),"https://huggingface.co/askmyteapot/GPT4-x-AlpacaDente2-30b-4bit")),(0,n.kt)("h2",f({},{id:"vicuna-13b-free-v11-05012023"}),"Vicuna 13B Free v1.1 (05/01/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"A work-in-progress, community driven attempt to make an unfiltered version of Vicuna. It currently has an early stopping bug, and a partial workaround has been posted on the repo's model card.\n\n>Filtering : Light\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU & CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/reeducator/vicuna-13b-free"}),"https://huggingface.co/reeducator/vicuna-13b-free"))))),(0,n.kt)("h2",f({},{id:"pygmalionmetharme-7b-04302023"}),"Pygmalion/Metharme 7B (04/30/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"Pygmalion 7B is a dialogue model that uses LLaMA-7B as a base. The dataset includes RP/ERP content. Metharme 7B is an experimental instruct-tuned variation, which can be guided using natural language like other instruct models.\n\nPygmalionAI intend to use the same dataset on the higher parameter LLaMA models. No ETA as of yet.\n\n>Filtering : None\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B Pygmalion/Metharme"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"XOR"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/PygmalionAI/"}),"https://huggingface.co/PygmalionAI/"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B Pygmalion GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/TehVenom/Pygmalion-7b-4bit-Q4_1-GGML"}),"Q4"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/waifu-workshop/pygmalion-7b-ggml-q5_0"}),"Q5"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/waifu-workshop/pygmalion-7b-ggml-q8_0"}),"Q8"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B Metharme GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/TehVenom/Metharme-7b-4bit-Q4_1-GGML"}),"Q4"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/waifu-workshop/metharme-7b-ggml-q5_1"}),"Q5"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B Pygmalion"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/TehVenom/Pygmalion-7b-4bit-GPTQ-Safetensors"}),"Q4 Triton"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/gozfarb/pygmalion-7b-4bit-128g-cuda"}),"Q4 CUDA 128gs"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"7B Metharme"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/TehVenom/Metharme-7b-4bit-GPTQ-Safetensors"}),"Q4 Triton"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/askmyteapot/metharme"}),"Q4 CUDA"))))),(0,n.kt)("h2",f({},{id:"gpt4-x-alpasta-30b-04292023"}),"GPT4-X-Alpasta 30B (04/29/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"An attempt at improving Open Assistant's performance as an instruct while retaining its excellent prose. The merge consists of Chansung's GPT4-Alpaca Lora and Open Assistant's native fine-tune.\n\nIt is an extremely coherent model for logic based instruct outputs. And while the prose is generally very good, it does suffer from the \"Assistant\" personality bleedthrough that plagues the OpenAssistant dataset, which can give you dry dialogue for creative writing/chatbot purposes. However, several accounts claim it's nowhere near as bad as OA's finetunes, and that the prose and coherence gains makes up for it.\n\n>Filtering : Medium\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B 4bit"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU & GPU CUDA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b-4bit"}),"https://huggingface.co/MetaIX/GPT4-X-Alpasta-30b-4bit"))))),(0,n.kt)("h2",f({},{id:"openassistant-llama-30b-sft-6-04232023"}),"OpenAssistant LLaMa 30B SFT 6 (04/23/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),'An open-source alternative to OpenAI\u2019s ChatGPT/GPT 3.5 Turbo. However, it seems to suffer from [overfitting](https://www.datarobot.com/wiki/overfitting/) and is heavily filtered. Not recommended for creative writing or chat bots, given the "assistant" personality constantly bleeds through, giving you dry dialogue.\n\n>Filtering : Heavy\n')),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"XOR"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor"}),"https://huggingface.co/OpenAssistant/oasst-sft-6-llama-30b-xor"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/MildlyAggressiveGoose1/ggml-oasst-sft-6-llama-30B-q4_2"}),"Q4"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Peeepy/llama-33b-oasst-4bit"}),"Q4 CUDA"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/Peeepy/llama-30b-oasst-4bit-128g"}),"Q4 CUDA 128gs"))))),(0,n.kt)("h2",f({},{id:"supercot-04222023"}),"SuperCOT (04/22/2023)"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"SuperCOT is a LoRA trained with the aim of making LLaMa follow prompts for Langchain better, by infusing chain-of-thought datasets, code explanations and instructions, snippets, logical deductions and Alpaca GPT-4 prompts.\n\nThough designed to improve Langchain, it's quite versatile and works very well for other tasks like creative writing and chatbots. The author also pruned a number of filters from the datasets. As of early May 2023, it's the most recommended model on /lmg/\n\n>Filtering : Light\n")),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Type"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"Original LoRA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LoRA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/kaiokendev/SuperCOT-LoRA"}),"https://huggingface.co/kaiokendev/SuperCOT-LoRA"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/camelids/llama-13b-supercot-ggml-q4_2"}),"Q4"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/camelids/llama-13b-supercot-ggml-q8_0"}),"Q8"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B GGML"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"CPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/camelids/llama-33b-supercot-ggml-q4_2"}),"Q4"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/camelids/llama-33b-supercot-ggml-q5_1"}),"Q5"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/camelids/llama-33b-supercot-ggml-q8_0"}),"Q8"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"13B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/ausboss/llama-13b-supercot-4bit-128g"}),"Q4 CUDA 128gs"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"30B"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),"GPU"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/tsumeone/llama-30b-supercot-4bit-cuda"}),"Q4 CUDA"),", ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/tsumeone/llama-30b-supercot-4bit-128g-cuda"}),"Q4 CUDA 128gs"))))),(0,n.kt)("h2",f({},{id:"previous-model-list"}),"Previous Model List"),(0,n.kt)("p",null,"!!! info"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{}),"The old rentry, retained for archiving purposes. Contains older and outdated models.\n")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://rentry.org/backupmdlist"}),"https://rentry.org/backupmdlist")),(0,n.kt)("hr",null),(0,n.kt)("h1",f({},{id:"models-for-llamacpp-ggml-format"}),"Models for ",(0,n.kt)("a",f({parentName:"h1"},{href:"https://github.com/ggerganov/llama.cpp"}),"llama.cpp")," (",(0,n.kt)("a",f({parentName:"h1"},{href:"https://github.com/ggerganov/ggml"}),"ggml")," format)"),(0,n.kt)("h2",f({},{id:"llama-quantized-4-bit-weights-ggml-q4_0"}),"LLaMA quantized 4-bit weights (ggml q4_0)"),(0,n.kt)("h4",f({},{id:"2023-03-31-torrent-magnet"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:481dee5424b7024433504803a90efd32dae40fdf&dn=LLaMA-ggml-4bit_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce"}),"2023-03-31 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"2dad53e70ca521fedcf9f9be5c26c15df602487a9c008bdafbb2bf8f946b6bf0  llama-7b-ggml-q4_0/ggml-model-q4_0.bin\n9cd4d6c1f5f42d5abf529c51bde3303991fba912ab8ed452adfd7c97a4be77d7  llama-13b-ggml-q4_0/ggml-model-q4_0.bin\ndaefbc6b1b644a75be0286ef865253ab3786e96a2c1bca8b71216b1751eee63e  llama-33b-ggml-q4_0/ggml-model-q4_0.bin\nd58a29c8403ecbd14258bbce07d90894fc5a8be25b9d359463c18f9f2ef96eb6  llama-65b-ggml-q4_0/ggml-model-q4_0.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("h2",f({},{id:"alpaca-quantized-4-bit-weights-ggml-q4_0"}),"Alpaca quantized 4-bit weights (ggml q4_0)"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 7B fine-tune from ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/chavinlo/alpaca-native/tree/062111ff2af99db24f466562b8eb7e7e4ad7566d"}),"chavinlo/alpaca-native")),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:d931a826b59443f4e543c18a25009b0ce8eabf39&dn=Alpaca-7B-ggml-4bit-native-finetune_2023-03-31&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce"}),"2023-03-31 torrent magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 7B merged with ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/tloen/alpaca-lora-7b/tree/28801eabf63a125cee9e46d8073fb13c7c8bd8b9"}),"tloen/alpaca-lora-7b")," LoRA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:694e206c1ce2780db673bdc2ecee78abcf228324&dn=Alpaca-7B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce"}),"2023-03-31 torrent magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 13B merged with ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/chansung/alpaca-lora-13b/tree/abcdddb2778cace16f184dc1dda0ecf21ade23bc"}),"chansung/alpaca-lora-13b")," LoRA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:31ad0f8e8da5d43bad83eeed94f24cca504330d1&dn=Alpaca-13B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce"}),"2023-03-31 torrent magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 33B merged with ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/chansung/alpaca-lora-30b/tree/bbbc77a38ad00a64780a76d119c783b6dc8200bd"}),"chansung/alpaca-lora-30b")," LoRA"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:1e8681e255ec3078ef84fe4cdecdc7abd8b2b6e5&dn=Alpaca-33B-ggml-4bit-LoRA-merged_2023-03-31&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce"}),"2023-03-31 torrent magnet"))))),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#instruction-mode-with-alpaca"}),"Tutorial link for llama.cpp"),"\nExample:\n",(0,n.kt)("inlineCode",{parentName:"p"},"./main --model ggml-model-q4_0.bin --file prompts/alpaca.txt --instruct --ctx_size 2048 --keep -1"),"\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"f5e264b10944c55a84810e8073dfdcd653fa8e47ff50ea043ec071051ac7821d  alpaca-7b-ggml-q4_0-native-finetune/ggml-model-q4_0.bin\nd9777baad5cf6a5d196e70867338d8cc3c7af68c7744e68de839a522983860d7  alpaca-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin\n3838aa32651c65948e289374abd71f6feab1a62a4921a648e30d979df86a4af3  alpaca-13b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin\n2267ed1dc0bf0d6d300ba292c25083c7fa5395f3726c7c68a49b2be19a64b349  alpaca-33b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("h2",f({},{id:"gpt4all-7b-quantized-4-bit-weights-ggml-q4_0"}),"GPT4All 7B quantized 4-bit weights (ggml q4_0)"),(0,n.kt)("h4",f({},{id:"2023-03-31-torrent-magnet-1"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:04584d8e5799c7838ccb987fae4f183936b9d744&dn=GPT4All-7B-ggml-4bit-lora-merged_2023-03-31&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce"}),"2023-03-31 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\nGPT4All can be used with llama.cpp in the same way as the other ",(0,n.kt)("inlineCode",{parentName:"p"},"ggml")," models.\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"9f6cd4830a3c45a86147c80a32888e7be8f8a489284c87cdb882a7cfe40940c1  gpt4all-unfiltered-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin\nde314c5ee155ac40a03ca3b3be85ba2b02aef9e9f083c411c0b4490689dd047e  gpt4all-7b-ggml-q4_0-lora-merged/ggml-model-q4_0.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("h2",f({},{id:"gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_0"}),"GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_0)"),(0,n.kt)("h4",f({},{id:"2023-04-01-torrent-magnet"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:f77827abd0cfb77399a0b281a1dbaeac5c386413&dn=GPT4-x-Alpaca-13B-ggml-4bit_2023-04-01&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce"}),"2023-04-01 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\nGPT4 x Alpaca can be used with llama.cpp in the same way as the other ",(0,n.kt)("inlineCode",{parentName:"p"},"ggml")," models.\nText generation with this version is faster compared to the ",(0,n.kt)("a",f({parentName:"p"},{href:"https://rentry.org/nur779#gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128"}),"GPTQ-quantized one"),".\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksum:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"e6b77ebf297946949b25b3c4b870f10cdc98fb9fcaa6d19cef4dda9021031580  gpt4-x-alpaca-13b-ggml-q4_0/ggml-model-q4_0.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://desuarchive.org/g/thread/92479457/#q92481589"}),"Model source")),(0,n.kt)("h2",f({},{id:"gpt4-x-alpaca-13b-quantized-4-bit-weights-ggml-q4_1-from-gptq-with-groupsize-128"}),"GPT4 x Alpaca 13B quantized 4-bit weights (ggml q4_1 from GPTQ with groupsize 128)"),(0,n.kt)("h4",f({},{id:"2023-04-01-torrent-magnet-1"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:6cdb6ab819b13b00928182eea72106824e335734&dn=GPT4-x-Alpaca-13B-ggml-4bit-from-GPTQ-128g_2023-04-01&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce"}),"2023-04-01 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\nGPT4 x Alpaca can be used with llama.cpp in the same way as the other ",(0,n.kt)("inlineCode",{parentName:"p"},"ggml")," models.\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksum:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"d4a640a1ce33009c244a361c6f87733aacbc2bea90e84d3c304a4c8be2bdf22d  gpt4-x-alpaca-13b-ggml-q4_1-from-gptq-4bit-128g/ggml-model-q4_1.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://desuarchive.org/g/thread/92479457/#q92481589"}),"Model source")),(0,n.kt)("h2",f({},{id:"vicuna-13b-quantized-4-bit-weights-ggml-q4_0"}),"Vicuna 13B quantized 4-bit weights (ggml q4_0)"),(0,n.kt)("h4",f({},{id:"2023-04-03-torrent-magnet"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:1e0c3dbeefe82483f81bd4e7ea959e4953c8081f&dn=Vicuna-13B-ggml-4bit-delta-merged_2023-04-03&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce"}),"2023-04-03 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\nVicuna can be used with llama.cpp in the same way as the other ",(0,n.kt)("inlineCode",{parentName:"p"},"ggml")," models.\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksum:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"f96689a13c581f53b616887b2efe82bbfbc5321258dbcfdbe69a22076a7da461  vicuna-13b-ggml-q4_0-delta-merged/ggml-model-q4_0.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/lmsys/vicuna-13b-delta/tree/da39ef5c586459f4d509bf7382475af584277e71"}),"Model source")),(0,n.kt)("h2",f({},{id:"openassistant-llama-13b-quantized-4-bit-weights-ggml-q4_0--q4_1"}),"OpenAssistant LLaMA 13B quantized 4-bit weights (ggml q4_0 & q4_1)"),(0,n.kt)("p",null,"!!! warning Note that this model is ",(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/dvruette/oasst-llama-13b-2-epochs/discussions/1#642ec79032e711e21aa11b60"}),"work-in-progress"),"."),(0,n.kt)("h4",f({},{id:"2023-04-07-torrent-magnet--huggingface-hub-direct-download"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:cad2f029978033f9c1487df3965546cc4d44489a&xt=urn:btmh:1220140702f43fbf90157db9531ad0454020bc212fddc48c7c30f593ec40d26eb19b&dn=oasst-llama-13b-ggml&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce"}),"2023-04-07 torrent magnet")," | ",(0,n.kt)("a",f({parentName:"h4"},{href:"https://huggingface.co/Black-Engineer/oasst-llama13b-ggml-q4/tree/main"}),"HuggingFace Hub direct download")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/ggerganov/llama.cpp#interactive-mode"}),"Tutorial link for llama.cpp"),"\n!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/LostRuins/koboldcpp#usage"}),"Tutorial link for koboldcpp")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"fe77206c7890ecd0824c7b6b6a6deab92e471366b2e4271c05ece9a686474ef6  ggml-model-q4_0.bin\n412da683b6ab0f710ce0adc8bc36db52bb92df96698558c5f2a1399af9bd0a78  ggml-model-q4_1.bin\n")),(0,n.kt)("p",null,"ggml model file magic: ",(0,n.kt)("inlineCode",{parentName:"p"},"0x67676a74")," (",(0,n.kt)("inlineCode",{parentName:"p"},"ggjt")," in hex)\nggml model file version: ",(0,n.kt)("inlineCode",{parentName:"p"},"1")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/dvruette/oasst-llama-13b-2-epochs"}),"Original model source"),"\n",(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/gozfarb/oasst-llama13b-4bit-128g"}),"GPTQ-quantized model source"),"\n",(0,n.kt)("a",f({parentName:"p"},{href:"https://desuarchive.org/g/thread/92596368/#q92601864"}),"Torrent source")),(0,n.kt)("hr",null),(0,n.kt)("h1",f({},{id:"models-for-huggingface-"}),"Models for HuggingFace \ud83e\udd17"),(0,n.kt)("p",null,"!!! danger Updated tokenizer and model configuration files can be found ",(0,n.kt)("a",f({parentName:"p"},{href:"https://rentry.org/544p2"}),"here"),".\nEnsure that your models have the appropriate JSON files within the same directory as the weights, otherwise text generation might be impacted by tokenization problems. The issues were addressed ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/huggingface/transformers/pull/22402"}),"here")," and ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/lm-sys/FastChat/pull/167"}),"here"),", but a manual update of both the ",(0,n.kt)("inlineCode",{parentName:"p"},"transformers")," library and your model configuration files is required."),(0,n.kt)("h2",f({},{id:"llama-float16-weights"}),"LLaMA float16 weights"),(0,n.kt)("h4",f({},{id:"2023-03-26-torrent-magnet--huggingface-hub-direct-downloads"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:496ee41a35f8d845f6d6cba11baa8b332f3c3318&dn=Safe-LLaMA-HF%20(3-26-23)&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"2023-03-26 torrent magnet")," | ",(0,n.kt)("a",f({parentName:"h4"},{href:"https://huggingface.co/Neko-Institute-of-Science"}),"HuggingFace Hub direct downloads")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#hugging-face-format-weights"}),"Tutorial link for Text generation web UI")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1484235789"}),"Torrent source and SHA256 checksums")),(0,n.kt)("h2",f({},{id:"vicuna-13b-float16-weights"}),"Vicuna 13B float16 weights"),(0,n.kt)("h4",f({},{id:"2023-04-03-torrent-magnet-1"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:a7fac57094561a63d53eed943f904abf24c6969d&dn=Vicuna-13B-HF-fp16-delta-merged_2023-04-03&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce"}),"2023-04-03 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#hugging-face-format-weights"}),"Tutorial link for Text generation web UI")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://huggingface.co/lmsys/vicuna-13b-delta/tree/da39ef5c586459f4d509bf7382475af584277e71"}),"Model source")),(0,n.kt)("h2",f({},{id:"llama-quantized-4-bit-weights-gptq-format-without-groupsize"}),"LLaMA quantized 4-bit weights (",(0,n.kt)("a",f({parentName:"h2"},{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa"}),"GPTQ")," format without groupsize)"),(0,n.kt)("h4",f({},{id:"2023-03-26-torrent-magnet"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:e88abf1b84290b162f00d3a9d79fb4f8719c2053&dn=LLaMA-HF-4bit&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"2023-03-26 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode"}),"Tutorial link for Text generation web UI")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"09841a1c4895e1da3b05c1bdbfb8271c6d43812661e4348c862ff2ab1e6ff5b3  llama-7b-4bit/llama-7b-4bit.safetensors\nedfa0b4060aae392b1e9df21fb60a97d78c9268ac6972e3888f6dc955ba0377b  llama-13b-4bit/llama-13b-4bit.safetensors\n4cb560746fe58796233159612d8d3c9dbdebdf6f0443b47be71643f2f91b8541  llama-30b-4bit/llama-30b-4bit.safetensors\n886ce814ed54c4bd6850e2216d5f198c49475210f8690f45dc63365d9aff3177  llama-65b-4bit/llama-65b-4bit.safetensors\n")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483891617"}),"Torrent source and more information")),(0,n.kt)("h2",f({},{id:"llama-quantized-4-bit-weights-gptq-format-with-groupsize-128"}),"LLaMA quantized 4-bit weights (",(0,n.kt)("a",f({parentName:"h2"},{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa"}),"GPTQ")," format with groupsize 128)"),(0,n.kt)("h4",f({},{id:"2023-03-26-torrent-magnet-1"}),(0,n.kt)("a",f({parentName:"h4"},{href:"magnet:?xt=urn:btih:88f7d9d2460ffcaf78b21e83012de00939eacb65&dn=LLaMA-HF-4bit-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce&tr=http%3a%2f%2fbt1.archive.org%3a6969%2fannounce"}),"2023-03-26 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode"}),"Tutorial link for Text generation web UI"),"\n",(0,n.kt)("inlineCode",{parentName:"p"},"Groupsize 128")," is a better choice for the 13B, 33B and 65B models, according to ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105"}),"this"),"."),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"ed8ec9c9f0ebb83210157ad0e3c5148760a4e9fd2acfb02cf00f8f2054d2743b  llama-7b-4bit-128g/llama-7b-4bit-128g.safetensors\nd3073ef1a2c0b441f95a5d4f8a5aa3b82884eef45d8997270619cb29bcc994b8  llama-13b-4bit-128g/llama-13b-4bit-128g.safetensors\n8b7d75d562938823c4503b956cb4b8af6ac0a5afbce2278566cc787da0f8f682  llama-30b-4bit-128g/llama-30b-4bit-128g.safetensors\nf1418091e3307611fb0a213e50a0f52c80841b9c4bcba67abc1f6c64c357c850  llama-65b-4bit-128g/llama-65b-4bit-128g.safetensors\n")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/pull/530#issuecomment-1483941105"}),"Torrent source and more information")),(0,n.kt)("h2",f({},{id:"alpaca-quantized-4-bit-weights-gptq-format-with-groupsize-128"}),"Alpaca quantized 4-bit weights (",(0,n.kt)("a",f({parentName:"h2"},{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa"}),"GPTQ")," format with groupsize 128)"),(0,n.kt)("table",null,(0,n.kt)("thead",{parentName:"table"},(0,n.kt)("tr",{parentName:"thead"},(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Model"),(0,n.kt)("th",f({parentName:"tr"},{align:null}),"Download"))),(0,n.kt)("tbody",{parentName:"table"},(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 7B fine-tune from ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/ozcur/alpaca-native-4bit"}),"ozcur/alpaca-native-4bit")," as safetensors"),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:90674fd4a3672c6eae5bf994634109bb75429e6b&dn=Alpaca-7B-GPTQ-4bit-128g-native-finetune_2023-03-29&tr=udp%3a%2f%2fopentracker.i2p.rocks%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.altrosky.nl%3a6969%2fannounce&tr=udp%3a%2f%2ftracker-udp.gbitt.info%3a80%2fannounce&tr=udp%3a%2f%2ftracker.opentrackr.org%3a1337%2fannounce&tr=udp%3a%2f%2ftracker.skynetcloud.site%3a6969%2fannounce&tr=udp%3a%2f%2ftracker2.dler.org%3a80%2fannounce&tr=udp%3a%2f%2ftracker.monitorit4.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.lelux.fi%3a6969%2fannounce&tr=udp%3a%2f%2ftracker1.bt.moack.co.kr%3a80%2fannounce&tr=udp%3a%2f%2ftracker.torrent.eu.org%3a451%2fannounce&tr=udp%3a%2f%2f9.rarbg.com%3a2810%2fannounce&tr=udp%3a%2f%2ftracker.theoks.net%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.moeking.me%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.tiny-vps.com%3a6969%2fannounce&tr=udp%3a%2f%2ftracker.openbittorrent.com%3a6969%2fannounce&tr=http%3a%2f%2ftracker.openbittorrent.com%3a80%2fannounce&tr=udp%3a%2f%2ftracker.dler.org%3a6969%2fannounce&tr=https%3a%2f%2fopentracker.i2p.rocks%3a443%2fannounce"}),"2023-03-29 torrent magnet"))),(0,n.kt)("tr",{parentName:"tbody"},(0,n.kt)("td",f({parentName:"tr"},{align:null}),"LLaMA 33B merged with ",(0,n.kt)("a",f({parentName:"td"},{href:"https://huggingface.co/baseten/alpaca-30b"}),"baseten/alpaca-30b")," LoRA by ",(0,n.kt)("a",f({parentName:"td"},{href:"https://desuarchive.org/g/thread/92351574/#q92356537"}),"an anon")),(0,n.kt)("td",f({parentName:"tr"},{align:null}),(0,n.kt)("a",f({parentName:"td"},{href:"magnet:?xt=urn:btih:81cf9b528cc80e390323f9ec50d4dfb4debcb490&dn=Alpaca%2030B%204bit%20groupsize%20128&tr=http%3A%2F%2Fbt2.archive.org%3A6969%2Fannounce"}),"2023-03-26 torrent magnet")," ","|"," ",(0,n.kt)("a",f({parentName:"td"},{href:"https://rentry.org/544p2#llama-33b"}),"extra config files"))))),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode"}),"Tutorial link for Text generation web UI")),(0,n.kt)("p",null,"SHA256 checksums:"),(0,n.kt)("pre",null,(0,n.kt)("code",f({parentName:"pre"},{className:"language-text"}),"17d6ba8f83be89f8dfa05cd4720cdd06b4d32c3baed79986e3ba1501b2305530  Alpaca-7B-GPTQ-4bit-128g-native-finetune_2023-03-29/alpaca-7b-4bit-128g-native-finetune.safetensors\na2f8d202ce61b1b612afe08c11f97133c1d56076d65391e738b1ab57c854ee05  Alpaca-30B-4bit-128g/alpaca-30b-hf-4bit.safetensors\n")),(0,n.kt)("h2",f({},{id:"vicuna-13b-quantized-4-bit--8-bit-weights-gptq-format-with-groupsize-128"}),"Vicuna 13B quantized 4-bit & 8-bit weights (",(0,n.kt)("a",f({parentName:"h2"},{href:"https://github.com/qwopqwop200/GPTQ-for-LLaMa"}),"GPTQ")," format with groupsize 128)"),(0,n.kt)("h5",f({},{id:"2023-04-03-torrent-magnet-2"}),(0,n.kt)("a",f({parentName:"h5"},{href:"magnet:?xt=urn:btih:f67d372a01c0b8e0162931623d6c55a5e6f34921&dn=Vicuna-13B-quantized-128g&tr=http%3a%2f%2fbt2.archive.org%3a6969%2fannounce"}),"2023-04-03 torrent magnet")),(0,n.kt)("p",null,"!!! info ",(0,n.kt)("a",f({parentName:"p"},{href:"https://github.com/oobabooga/text-generation-webui/wiki/LLaMA-model#4-bit-mode"}),"Tutorial link for Text generation web UI")),(0,n.kt)("p",null,(0,n.kt)("a",f({parentName:"p"},{href:"https://desuarchive.org/g/thread/92531914#92536953"}),"Torrent source"),"\n",(0,n.kt)("a",f({parentName:"p"},{href:"https://rentry.org/544p2#llama-13b"}),"Extra config files")))}y.isMDXComponent=!0}}]);